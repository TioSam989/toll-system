# Portuguese Toll Tariffs Scraper

Python application for scraping Portuguese toll tariffs from official transportation websites.

## ğŸ—ï¸ Project Structure

```
toll-system/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ scrapers/          # Web scrapers
â”‚   â”‚   â”œâ”€â”€ base_scraper.py
â”‚   â”‚   â”œâ”€â”€ brisa_scraper.py
â”‚   â”‚   â””â”€â”€ portugal_tolls_scraper.py
â”‚   â”œâ”€â”€ parsers/           # Data parsers
â”‚   â”‚   â””â”€â”€ pdf_parser.py
â”‚   â””â”€â”€ utils/             # Utilities
â”‚       â””â”€â”€ data_exporter.py
â”œâ”€â”€ config/                # Configuration
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ data/                  # Data storage
â”‚   â”œâ”€â”€ pdfs/             # Downloaded PDFs
â”‚   â”œâ”€â”€ parsed/           # Parsed data
â”‚   â””â”€â”€ exports/          # Final exports
â”œâ”€â”€ logs/                 # Application logs
â”œâ”€â”€ tests/                # Unit tests
â”œâ”€â”€ main.py              # Main entry point
â””â”€â”€ requirements.txt     # Dependencies
```

## Features

- Modular architecture with clean separation of concerns
- Multiple data sources: Brisa Concessao, Portugal Tolls, Infraestruturas de Portugal
- PDF processing with automatic download and parsing
- Location-based organization of toll data
- Export formats: CSV, JSON with location grouping
- Error handling with fallback mechanisms
- Structured logging with file output

## ğŸ“¦ Installation

1. **Clone the repository**
```bash
git clone <https://github.com/TioSam989/toll-system>
cd toll-system
```

2. **Install Browser Driver (Required for Selenium)**

### For Debian/Ubuntu WSL (Tested & Working):
```bash
# Step 1: Update package repositories
sudo apt update

# Step 2: Install Chromium browser
sudo apt install chromium

# Step 3: Clear any broken webdriver cache
rm -rf ~/.wdm

# Step 4: Install system ChromeDriver (recommended)
sudo apt install chromium-driver

# Step 5: Verify installation
which chromium
/usr/bin/chromedriver --version
```

### Alternative Methods (if above fails):
```bash
# Method 1: Install Chromium via snap
sudo apt install snapd
sudo snap install chromium

# Method 2: Manual Chrome installation
cd /tmp
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo dpkg -i google-chrome-stable_current_amd64.deb
sudo apt-get install -f

# Method 3: Install Firefox as fallback
sudo apt install firefox
```

### For Kali Linux:
```bash
# Kali has limited repositories, use Firefox
sudo apt update
sudo apt install firefox
```

### Troubleshooting:
- If you get "Exec format error", clear webdriver cache: `rm -rf ~/.wdm`
- Script automatically detects Chrome, Chromium, or Firefox
- System drivers are preferred over webdriver-manager downloads

3. **Install Python dependencies**
```bash
pip install -r requirements.txt
```

4. **Run the application**
```bash
python main.py
```

## ğŸ¯ Usage

### Basic Usage
```bash
python main.py
```

### Programmatic Usage
```python
from src.scrapers.brisa_scraper import BrisaScraper
from src.parsers.pdf_parser import PDFParser
from src.utils.data_exporter import DataExporter

# Initialize components
scraper = BrisaScraper()
parser = PDFParser()
exporter = DataExporter()

# Scrape data
data = scraper.scrape()

# Export results
exporter.export_to_json(data)
```

## ğŸ“Š Output Files

The application generates organized output in the `data/` directory:

- **`data/pdfs/`**: Downloaded PDF files
- **`data/parsed/`**: Parsed toll data organized by location
- **`data/exports/`**: Final CSV and JSON exports
- **`logs/`**: Application logs

### Location-Based JSON Structure
```json
{
  "A1 Lisboa-Porto": [
    {
      "route": "A1 Lisboa-Porto",
      "vehicle_class": "Class 1",
      "price": "22.85",
      "currency": "EUR"
    }
  ]
}
```

## ğŸ”§ Configuration

Modify `config/settings.py` to customize:
- Output directories
- Scraper timeouts
- Website URLs
- Logging configuration

## ğŸ§ª Testing

### Unit Tests
```bash
python -m pytest tests/
```

### TestCafe E2E Tests (TypeScript)
```bash
# Install dependencies
npm install

# Run all TestCafe tests
npm run test:e2e

# Run tests in headless mode
npm run test:e2e:headless

# Run tests in Firefox
npm run test:e2e:firefox

# Run specific test file
testcafe chrome tests/testcafe/initial-test.ts
```

### CLI Toll Calculator
```bash
# Calculate toll between two addresses
npm run toll-calc "Lisboa" "Porto"
npm run toll-calc "1250-161" "4000-322"
npm run toll-calc "Coimbra" "Aveiro"

# Perfect for Raspberry Pi automation
npm run toll-calc "Your Origin" "Your Destination"
```

## ğŸ“ Logging

Logs are written to:
- Console (INFO level)
- `logs/toll_scraper.log` (INFO level)

## ğŸ› ï¸ Development

### Adding New Scrapers
1. Create new scraper class inheriting from `BaseScraper`
2. Implement the `scrape()` method
3. Add to main execution flow

### Adding New Parsers
1. Create parser class in `src/parsers/`
2. Implement parsing logic
3. Integrate with data exporter

## ğŸ“‹ Requirements

- Python 3.7+
- Browser: Chromium (recommended), Chrome, or Firefox
- ChromeDriver or GeckoDriver (auto-installed)
- Internet connection

## ğŸ”’ Legal Notice

This scraper is intended for educational and research purposes. Ensure compliance with website terms of service and robots.txt files before use.

## ğŸ“„ License

This project is licensed under the MIT License.